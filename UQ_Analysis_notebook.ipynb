{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gjhVTJQSvTO"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.layers import InputSpec, Dense, Wrapper, Input, concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class ConcreteDropout(Wrapper):\n",
        "    \"\"\"This wrapper allows to learn the dropout probability for any given input Dense layer.\n",
        "    ```python\n",
        "        # as the first layer in a model\n",
        "        model = Sequential()\n",
        "        model.add(ConcreteDropout(Dense(8), input_shape=(16)))\n",
        "        # now model.output_shape == (None, 8)\n",
        "        # subsequent layers: no need for input_shape\n",
        "        model.add(ConcreteDropout(Dense(32)))\n",
        "        # now model.output_shape == (None, 32)\n",
        "    ```\n",
        "    `ConcreteDropout` can be used with arbitrary layers which have 2D\n",
        "    kernels, not just `Dense`. However, Conv2D layers require different\n",
        "    weighing of the regulariser (use SpatialConcreteDropout instead).\n",
        "    # Arguments\n",
        "        layer: a layer instance.\n",
        "        weight_regularizer:\n",
        "            A positive number which satisfies\n",
        "                $weight_regularizer = l**2 / (\\tau * N)$\n",
        "            with prior lengthscale l, model precision $\\tau$ (inverse observation noise),\n",
        "            and N the number of instances in the dataset.\n",
        "            Note that kernel_regularizer is not needed.\n",
        "        dropout_regularizer:\n",
        "            A positive number which satisfies\n",
        "                $dropout_regularizer = 2 / (\\tau * N)$\n",
        "            with model precision $\\tau$ (inverse observation noise) and N the number of\n",
        "            instances in the dataset.\n",
        "            Note the relation between dropout_regularizer and weight_regularizer:\n",
        "                $weight_regularizer / dropout_regularizer = l**2 / 2$\n",
        "            with prior lengthscale l. Note also that the factor of two should be\n",
        "            ignored for cross-entropy loss, and used only for the eculedian loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layer, weight_regularizer=0, dropout_regularizer=1e-5,\n",
        "                 init_min=0.1, init_max=0.1, is_mc_dropout=True, **kwargs):\n",
        "        assert 'kernel_regularizer' not in kwargs\n",
        "        super(ConcreteDropout, self).__init__(layer, **kwargs)\n",
        "        self.weight_regularizer = weight_regularizer\n",
        "        self.dropout_regularizer = dropout_regularizer\n",
        "        self.is_mc_dropout = is_mc_dropout\n",
        "        self.supports_masking = True\n",
        "        self.p_logit = None\n",
        "        self.init_min = np.log(init_min) - np.log(1. - init_min)\n",
        "        self.init_max = np.log(init_max) - np.log(1. - init_max)\n",
        "\n",
        "    def build(self, input_shape=None):\n",
        "        self.input_spec = InputSpec(shape=input_shape)\n",
        "        if not self.layer.built:\n",
        "            self.layer.build(input_shape)\n",
        "            self.layer.built = True\n",
        "        super(ConcreteDropout, self).build()\n",
        "\n",
        "        # initialise p\n",
        "        self.p_logit = self.add_weight(name='p_logit',\n",
        "                                       shape=(1,),\n",
        "                                       initializer=tf.random_uniform_initializer(self.init_min, self.init_max),\n",
        "                                       dtype=tf.dtypes.float32,\n",
        "                                       trainable=True)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return self.layer.compute_output_shape(input_shape)\n",
        "\n",
        "    def concrete_dropout(self, x, p):\n",
        "        \"\"\"\n",
        "        Concrete dropout - used at training time (gradients can be propagated)\n",
        "        :param x: input\n",
        "        :return:  approx. dropped out input\n",
        "        \"\"\"\n",
        "        eps = 1e-07\n",
        "        temp = 0.1\n",
        "\n",
        "        unif_noise = tf.random.uniform(shape=tf.shape(x))\n",
        "        drop_prob = (\n",
        "            tf.math.log(p + eps)\n",
        "            - tf.math.log(1. - p + eps)\n",
        "            + tf.math.log(unif_noise + eps)\n",
        "            - tf.math.log(1. - unif_noise + eps)\n",
        "        )\n",
        "        drop_prob = tf.math.sigmoid(drop_prob / temp)\n",
        "        random_tensor = 1. - drop_prob\n",
        "\n",
        "        retain_prob = 1. - p\n",
        "        x *= random_tensor\n",
        "        x /= retain_prob\n",
        "        return x\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        p = tf.math.sigmoid(self.p_logit)\n",
        "\n",
        "        # initialise regulariser / prior KL term\n",
        "        input_dim = inputs.shape[-1]  # last dim\n",
        "        weight = self.layer.kernel\n",
        "        kernel_regularizer = self.weight_regularizer * tf.reduce_sum(tf.square(weight)) / (1. - p)\n",
        "        dropout_regularizer = p * tf.math.log(p) + (1. - p) * tf.math.log(1. - p)\n",
        "        dropout_regularizer *= self.dropout_regularizer * input_dim\n",
        "        regularizer = tf.reduce_sum(kernel_regularizer + dropout_regularizer)\n",
        "        if self.is_mc_dropout:\n",
        "            return self.layer.call(self.concrete_dropout(inputs, p)), regularizer\n",
        "        else:\n",
        "            def relaxed_dropped_inputs():\n",
        "                return self.layer.call(self.concrete_dropout(inputs, p)), regularizer\n",
        "\n",
        "            return tf.keras.backend.in_train_phase(relaxed_dropped_inputs,\n",
        "                                                   self.layer.call(inputs),\n",
        "                                                   training=training), regularizer\n",
        "\n",
        "\n",
        "def mse_loss(true, pred):\n",
        "    n_outputs = pred.shape[1] // 2\n",
        "    mean = pred[:, :n_outputs]\n",
        "    return tf.reduce_mean((true - mean) ** 2, -1)\n",
        "\n",
        "\n",
        "def heteroscedastic_loss(true, pred):\n",
        "    n_outputs = pred.shape[1] // 2\n",
        "    mean = pred[:, :n_outputs]\n",
        "    log_var = pred[:, n_outputs:]\n",
        "    precision = tf.math.exp(-log_var)\n",
        "    return tf.reduce_sum(precision * (true - mean) ** 2. + log_var, -1)\n",
        "\n",
        "\n",
        "def make_model(n_features, n_outputs, n_nodes=100, dropout_reg=1e-5, wd=0):\n",
        "    losses = []\n",
        "    inp = Input(shape=(n_features,))\n",
        "    x = inp\n",
        "    x, loss = ConcreteDropout(Dense(n_nodes, activation='relu'),\n",
        "                              weight_regularizer=wd, dropout_regularizer=dropout_reg)(x)\n",
        "    losses.append(loss)\n",
        "    x, loss = ConcreteDropout(Dense(n_nodes, activation='relu'),\n",
        "                              weight_regularizer=wd, dropout_regularizer=dropout_reg)(x)\n",
        "    losses.append(loss)\n",
        "    x, loss = ConcreteDropout(Dense(n_nodes, activation='relu'),\n",
        "                              weight_regularizer=wd, dropout_regularizer=dropout_reg)(x)\n",
        "    losses.append(loss)\n",
        "    mean, loss = ConcreteDropout(Dense(n_outputs), weight_regularizer=wd, dropout_regularizer=dropout_reg)(x)\n",
        "    losses.append(loss)\n",
        "    log_var, loss = ConcreteDropout(Dense(n_outputs), weight_regularizer=wd, dropout_regularizer=dropout_reg)(x)\n",
        "    losses.append(loss)\n",
        "    out = concatenate([mean, log_var])\n",
        "    model = Model(inp, out)\n",
        "    for loss in losses:\n",
        "        model.add_loss(loss)\n",
        "\n",
        "    model.compile(optimizer=optimizers.Adam(), loss=heteroscedastic_loss, metrics=[mse_loss])\n",
        "    assert len(model.layers[1].trainable_weights) == 3  # kernel, bias, and dropout prob\n",
        "    assert len(model.losses) == 5, f'{len(model.losses)} is not 5'  # a loss for each Concrete Dropout layer\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7VHuVXCSwMW"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow_probability.python.internal import tensorshape_util\n",
        "\n",
        "tfk = tf.keras\n",
        "tfkl = tf.keras.layers\n",
        "tfpl = tfp.layers\n",
        "tfd = tfp.distributions\n",
        "\n",
        "n_train = 90000\n",
        "\n",
        "\n",
        "class MeanMetricWrapper(tfk.metrics.Mean):\n",
        "    # code by @mcourteaux from https://github.com/tensorflow/probability/issues/742#issuecomment-580433644\n",
        "    def __init__(self, fn, name=None, dtype=None, **kwargs):\n",
        "        super(MeanMetricWrapper, self).__init__(name=name, dtype=dtype)\n",
        "        self._fn = fn\n",
        "        self._fn_kwargs = kwargs\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        matches = self._fn(y_true, y_pred, **self._fn_kwargs)\n",
        "        return super(MeanMetricWrapper, self).update_state(\n",
        "            matches, sample_weight=sample_weight)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {}\n",
        "        for k, v in self._fn_kwargs.items():\n",
        "            config[k] = K.eval(v) if is_tensor_or_variable(v) else v\n",
        "        base_config = super(MeanMetricWrapper, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "def scaled_kl_fn(a, b, _):\n",
        "    \"\"\"\n",
        "    idea from\n",
        "    https://github.com/google-research/google-research/blob/9645220c865ab5603b377e6a98265631ece61d44/uq_benchmark_2019/uq_utils.py\n",
        "    https://arxiv.org/pdf/1906.02530.pdf\n",
        "    :param a: distribution\n",
        "    :param b: distribution\n",
        "    :return: scaled kl divergence\n",
        "    \"\"\"\n",
        "    return tfd.kl_divergence(a, b) / n_train\n",
        "\n",
        "\n",
        "def mmd_from_dists(a, b, _):\n",
        "    p = a.distribution\n",
        "    q = b.distribution\n",
        "\n",
        "    num_reduce_dims = (tensorshape_util.rank(a.event_shape) -\n",
        "                       tensorshape_util.rank(p.event_shape))\n",
        "    gamma_sq = 0.5\n",
        "    reduce_dims = [-i - 1 for i in range(0, num_reduce_dims)]\n",
        "    for i in reduce_dims:\n",
        "        gamma_sq *= a.event_shape[i]\n",
        "\n",
        "    sigma_p = tf.convert_to_tensor(tf.square(p.scale))\n",
        "    sigma_q = tf.convert_to_tensor(tf.square(q.scale))\n",
        "    scale_pp = gamma_sq + 2 * sigma_p\n",
        "    scale_qq = gamma_sq + 2 * sigma_q\n",
        "    scale_cr = gamma_sq + sigma_p + sigma_q\n",
        "\n",
        "    return tf.reduce_sum(\n",
        "        tf.math.sqrt(gamma_sq / scale_pp) + tf.math.sqrt(gamma_sq / scale_qq)\n",
        "        - 2 * tf.math.sqrt(gamma_sq / scale_cr) * tf.math.exp(\n",
        "            -0.5 * tf.math.squared_difference(p.loc, q.loc) / scale_cr),\n",
        "        axis=reduce_dims)\n",
        "\n",
        "\n",
        "def negloglik(y_data, rv_y):\n",
        "    return -rv_y.log_prob(y_data)\n",
        "\n",
        "\n",
        "def negloglik_met(y_true, y_pred):\n",
        "    return tf.reduce_mean(-y_pred.log_prob(tf.cast(y_true, tf.float32)))\n",
        "\n",
        "\n",
        "def mlp(hidden_dim=100, n_layers=3, n_inputs=13, dropout_rate=0, loss='mse'):\n",
        "    input_data = tfkl.Input((n_inputs,))\n",
        "    x = input_data\n",
        "    for _ in range(n_layers):\n",
        "        x = tfkl.Dense(hidden_dim, activation='relu')(x)\n",
        "        if dropout_rate > 0:\n",
        "            x = tfkl.Dropout(dropout_rate)(x)\n",
        "\n",
        "    if loss == 'mse':\n",
        "        x = tfkl.Dense(1)(x)\n",
        "        model = tfk.Model(input_data, x)\n",
        "        model.compile(loss='mean_squared_error', optimizer=tf.optimizers.Adam())\n",
        "    elif loss == 'nll':\n",
        "        x = tfkl.Dense(2)(x)\n",
        "        x = tfpl.DistributionLambda(lambda t: tfd.Normal(loc=t[..., :1],\n",
        "                                                         scale=1e-3 + tf.math.softplus(t[..., 1:])))(x)\n",
        "        model = tfk.Model(input_data, x)\n",
        "        model.compile(optimizer=tf.optimizers.Adam(), loss=negloglik, metrics=['mse'])\n",
        "    else:\n",
        "        raise ValueError(f'Loss {loss} not implemented.')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def mlp_flipout(hidden_dim=100, n_layers=3, n_inputs=13, dropout_rate=0, kernel='kl'):\n",
        "    input_img = tfkl.Input(n_inputs)\n",
        "    x = input_img\n",
        "    if kernel == 'kl':\n",
        "        kernel_fn = scaled_kl_fn\n",
        "    elif kernel == 'mmd':\n",
        "        kernel_fn = mmd_from_dists\n",
        "    else:\n",
        "        raise ValueError(f'Kernel {kernel} not defined!')\n",
        "\n",
        "    for _ in range(n_layers):\n",
        "        x = tfpl.DenseFlipout(hidden_dim, activation='relu', kernel_divergence_fn=kernel_fn)(x)\n",
        "        if dropout_rate > 0:\n",
        "            x = tfkl.Dropout(dropout_rate)(x)\n",
        "    x = tfpl.DenseFlipout(2, kernel_divergence_fn=kernel_fn)(x)\n",
        "    x = tfpl.DistributionLambda(lambda t: tfd.Normal(loc=t[..., :1],\n",
        "                                                     scale=1e-3 + tf.math.softplus(t[..., 1:])))(x)\n",
        "    model = tfk.Model(input_img, x)\n",
        "\n",
        "    model.compile(optimizer=tf.optimizers.Adam(learning_rate=1e-4), loss=negloglik,\n",
        "                  metrics=['mse', MeanMetricWrapper(negloglik_met, name='nll')])\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cFqVCT7U440"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.special import gamma\n",
        "\n",
        "\n",
        "def pendulum(n=10000, n_t=10, g_range=None, theta_range=None, ell_range=None, m_range=None, t_spread=None,\n",
        "             ell_spread=None, seed=42):\n",
        "    if g_range is None:\n",
        "        g_range = [5, 15]\n",
        "    if theta_range is None:\n",
        "        theta_range = [5, 15]\n",
        "    if ell_range is None:\n",
        "        ell_range = [0.2, 0.8]\n",
        "    if m_range is None:\n",
        "        m_range = [0.02, 0.1]\n",
        "    if t_spread is None:\n",
        "        t_spread = [0.03, 0.03]\n",
        "    if ell_spread is None:\n",
        "        ell_spread = [0., 0.]\n",
        "\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    g = (g_range[1] - g_range[0]) * np.random.rand(n) + g_range[0]\n",
        "    theta = ((theta_range[1] - theta_range[0]) * np.random.rand(n) + theta_range[0]).reshape((n, 1))\n",
        "    ell = ((ell_range[1] - ell_range[0]) * np.random.rand(n) + ell_range[0]).reshape((n, 1))\n",
        "    m = ((m_range[1] - m_range[0]) * np.random.rand(n) + m_range[0]).reshape((n, 1))\n",
        "    t = (2 * np.pi * np.sqrt(ell / g.reshape((n, 1))))\n",
        "\n",
        "    t_scales = np.random.uniform(t_spread[0], t_spread[1], n)\n",
        "    t_scales = np.repeat(t_scales, n_t).reshape((n, n_t))\n",
        "    t_spreads = np.random.normal(scale=t_scales, size=(n, n_t))\n",
        "    t_sigma = t_spreads * t\n",
        "    t = t + t_sigma\n",
        "\n",
        "    ell_scales = np.random.uniform(ell_spread[0], ell_spread[1], n).reshape((n, 1))\n",
        "    ell_spreads = np.random.normal(scale=ell_scales, size=(n, 1))\n",
        "    ell_sigma = ell_spreads * ell\n",
        "    ell = ell + ell_sigma\n",
        "\n",
        "    feat = np.concatenate([theta, ell, m, t], axis=1)\n",
        "    y = g\n",
        "\n",
        "    # statistical uncertainty\n",
        "    delta_t = np.std(t_sigma, axis=1) / np.sqrt(2) * gamma((n_t-1)/2) / gamma(n_t/2)\n",
        "    mean_t = np.mean(t, axis=1)\n",
        "    ell = ell.reshape(n, )\n",
        "    delta_ell = ell * ell_scales.reshape(n, )\n",
        "    calc_y = 4 * np.pi ** 2 * ell / mean_t ** 2\n",
        "    delta_y = 4 * np.pi ** 2 / mean_t ** 2 * np.sqrt((2 * ell * delta_t / mean_t) ** 2 + delta_ell ** 2)\n",
        "\n",
        "    return feat, y, calc_y, delta_y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hr_9ebaBWXio"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "def scale(train_set, val_set, test_set=None, min_val=0, max_val=1, gap=0):\n",
        "    scaler = MinMaxScaler((min_val+gap, max_val-gap))\n",
        "\n",
        "    if train_set.ndim == 1:\n",
        "        train_set = train_set.reshape(-1, 1)\n",
        "        val_set = val_set.reshape(-1, 1)\n",
        "        if test_set is not None:\n",
        "            test_set = test_set.reshape(-1, 1)\n",
        "\n",
        "    scaler.fit(train_set)\n",
        "    train_set = scaler.transform(train_set)\n",
        "    val_set = scaler.transform(val_set)\n",
        "    if test_set is not None:\n",
        "        test_set = scaler.transform(test_set)\n",
        "        return scaler, train_set, val_set, test_set\n",
        "    return scaler, train_set, val_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxCvAD_PU8or"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "val_proportion = 0.1\n",
        "n_models = 10\n",
        "n_neurons = 100\n",
        "\n",
        "\n",
        "def main(model_type, t_spread_min, t_spread_max, ell_spread_min, ell_spread_max, n, n_test, n_epochs, data_dir):\n",
        "    # Generate data\n",
        "    feat, y, _, _ = pendulum(n=n, t_spread=[t_spread_min, t_spread_max], ell_spread=[ell_spread_min, ell_spread_max])\n",
        "\n",
        "    # Set up data\n",
        "    x_train, x_val, y_train, y_val = train_test_split(feat, y, test_size=val_proportion, random_state=42)\n",
        "    x_scaler, x_train, x_val = scale(x_train, x_val)\n",
        "    y_scaler, y_train, y_val = scale(y_train, y_val)\n",
        "\n",
        "    t_range_str = f'trange{int(100*t_spread_min)}to{int(100*t_spread_max)}'\n",
        "    model_name = f'{model_type}_{t_range_str}_{n_epochs}ep'\n",
        "\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "    if not os.path.isfile(f'{data_dir}x_scaler_{t_range_str}.pkl'):\n",
        "        with open(f'{data_dir}x_scaler_{t_range_str}.pkl', 'wb') as file_pi:\n",
        "            pickle.dump(x_scaler, file_pi)\n",
        "        with open(f'{data_dir}y_scaler_{t_range_str}.pkl', 'wb') as file_pi:\n",
        "            pickle.dump(y_scaler, file_pi)\n",
        "\n",
        "    # train and save models\n",
        "    model_number = 1\n",
        "    while os.path.isfile(f'{data_dir}model_{model_name}_{str(model_number).zfill(3)}.h5'):\n",
        "        model_number += 1\n",
        "\n",
        "    if model_type == 'de':\n",
        "        models = [mlp(loss='nll') for _ in range(n_models)]\n",
        "    elif model_type == 'cd':\n",
        "        n_features = x_train.shape[1]\n",
        "        n_outputs = y_train.shape[1]\n",
        "        dropout_reg = 2. / n\n",
        "        models = [make_model(n_features, n_outputs, n_neurons, dropout_reg)]\n",
        "    elif model_type == 'bnn':\n",
        "        models = [mlp_flipout()]\n",
        "    else:\n",
        "        raise ValueError(f'Model type {model_type} not recognized!')\n",
        "\n",
        "    for j, mod in enumerate(models):\n",
        "        print(f'Model {j+1}')\n",
        "        history = mod.fit(x_train, y_train, epochs=n_epochs, validation_data=(x_val, y_val))\n",
        "        mod.save_weights(f'{data_dir}model_{model_name}_{str(model_number+j).zfill(3)}.h5')\n",
        "        with open(f'{data_dir}history_{model_name}_{str(model_number+j).zfill(3)}.pkl', 'wb') as file_pi:\n",
        "            pickle.dump(history.history, file_pi)\n",
        "\n",
        "    # Generate test set\n",
        "    feat_test, _, _, _ = pendulum(n=n_test, t_spread=[t_spread_min, t_spread_max],\n",
        "                                  ell_spread=[ell_spread_min, ell_spread_max], seed=666)\n",
        "    feat_test = x_scaler.transform(feat_test)\n",
        "\n",
        "    # make predictions\n",
        "    if model_type == 'de':\n",
        "        y_pred = []\n",
        "        for model in models:\n",
        "            y_pred.append(model(feat_test.astype('float32')))\n",
        "    elif model_type == 'cd':\n",
        "        y_pred = np.array([models[0].predict(feat_test) for _ in range(n_models)])\n",
        "    elif model_type == 'bnn':\n",
        "        y_pred = [models[0](feat_test.astype('float32')) for _ in range(n_models)]\n",
        "\n",
        "    if model_type == 'de' or model_type == 'bnn':\n",
        "        y_pred_val = [pred.loc.numpy() for pred in y_pred]\n",
        "        y_pred_unc = [pred.scale.numpy() for pred in y_pred]\n",
        "    elif model_type == 'cd':\n",
        "        y_pred_val = y_pred[:, :, :1]\n",
        "        y_pred_unc = np.sqrt(np.exp(y_pred[:, :, 1:]))\n",
        "\n",
        "    y_pred_val_resc = [y_scaler.inverse_transform(y) for y in y_pred_val]\n",
        "    y_pred_unc_resc = [y / y_scaler.scale_[0] for y in y_pred_unc]\n",
        "\n",
        "    y_pred_val_resc = np.array(y_pred_val_resc).reshape((n_models, n_test))\n",
        "    y_pred_unc_resc = np.array(y_pred_unc_resc).reshape((n_models, n_test))\n",
        "\n",
        "    y_pred_mean = np.mean(y_pred_val_resc, axis=0)\n",
        "    y_pred_ep_unc = np.std(y_pred_val_resc, axis=0)\n",
        "    y_pred_al_unc = np.sqrt(np.mean(y_pred_unc_resc * y_pred_unc_resc, axis=0))\n",
        "    y_pred_unc = np.sqrt(y_pred_al_unc ** 2 + y_pred_ep_unc ** 2)\n",
        "\n",
        "    np.save(f'{data_dir}y_pred_test_{model_name}_{str(model_number).zfill(3)}.npy', y_pred_mean)\n",
        "    np.save(f'{data_dir}y_pred_test_alunc_{model_name}_{str(model_number).zfill(3)}.npy', y_pred_al_unc)\n",
        "    np.save(f'{data_dir}y_pred_test_epunc_{model_name}_{str(model_number).zfill(3)}.npy', y_pred_ep_unc)\n",
        "    np.save(f'{data_dir}y_pred_test_prunc_{model_name}_{str(model_number).zfill(3)}.npy', y_pred_unc)\n",
        "\n",
        "\n",
        "t_max = (0.05, 0.1, 0.2) # independent of model\n",
        "t_spread_max = (0.1, 0.1, 0.1)\n",
        "\n",
        "model_type = ('cd', 'bnn', 'de')\n",
        "t_spread_min = (0.01, 0.01, 0.01)\n",
        "t_spread_max = (0.1, 0.1, 0.1)\n",
        "ell_spread_min = (0.02, 0.02, 0.02)\n",
        "ell_spread_max = (0.02, 0.02, 0.02)\n",
        "n = (90000, 90000, 90000)\n",
        "n_test = (10000, 10000, 10000)\n",
        "n_epochs = (100, 100, 20)\n",
        "data_dir = ('data/', 'data/', 'data/')\n",
        "\n",
        "for i in range(3):\n",
        "  args = {'model_type': model_type[i],\n",
        "          't_spread_min': t_spread_min[i],\n",
        "          't_spread_max': t_spread_max[i],\n",
        "          'ell_spread_min': ell_spread_min[i],\n",
        "          'ell_spread_max': ell_spread_max[i],\n",
        "          'n': n[i],\n",
        "          'n_test': n_test[i],\n",
        "          'n_epochs': n_epochs[i],\n",
        "          'data_dir': data_dir[i]}\n",
        "  main(**args)\n",
        "%tb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "n_models = 10\n",
        "n_neurons = 100\n",
        "\n",
        "def main(model_type, model_number, t_spread_min, t_spread_max, ell_spread_min, ell_spread_max, ell_range_min,\n",
        "         ell_range_max, g_range_min, g_range_max, n_test, n_epochs, data_dir):\n",
        "    t_range_str = f'trange{int(100 * t_spread_min)}to{int(100 * t_spread_max)}'\n",
        "    model_name = f'{model_type}_{t_range_str}_{n_epochs}ep'\n",
        "\n",
        "    test_name = ''\n",
        "    if ell_range_min != 2 or ell_range_max != 8:\n",
        "        test_name += f'_ell{int(ell_range_min)}to{int(ell_range_max)}'\n",
        "    if g_range_min != 5. or g_range_max != 15.:\n",
        "        test_name += f'_g{int(g_range_min)}to{int(g_range_max)}'\n",
        "\n",
        "    ell_range_min /= 10.\n",
        "    ell_range_max /= 10.\n",
        "\n",
        "    # Generate data\n",
        "    feat_test, _, _, _ = pendulum(n=n_test, t_spread=[t_spread_min, t_spread_max],\n",
        "                                  ell_range=[ell_range_min, ell_range_max], g_range=[g_range_min, g_range_max],\n",
        "                                  ell_spread=[ell_spread_min, ell_spread_max], seed=666)\n",
        "\n",
        "    with open(f'{data_dir}x_scaler_{t_range_str}.pkl', 'rb') as file_pi:\n",
        "        x_scaler = pickle.load(file_pi)\n",
        "    with open(f'{data_dir}y_scaler_{t_range_str}.pkl', 'rb') as file_pi:\n",
        "        y_scaler = pickle.load(file_pi)\n",
        "\n",
        "    feat_test = x_scaler.transform(feat_test)\n",
        "\n",
        "    if model_type == 'de':\n",
        "        models = [mlp(loss='nll') for _ in range(n_models)]\n",
        "    elif model_type == 'cd':\n",
        "        n_features = feat_test.shape[1]\n",
        "        n_outputs = 1\n",
        "        models = [make_model(n_features, n_outputs, n_neurons)]\n",
        "    elif model_type == 'bnn':\n",
        "        models = [mlp_flipout()]\n",
        "    else:\n",
        "        raise ValueError(f'Model type {model_type} not recognized!')\n",
        "\n",
        "    for j, mod in enumerate(models):\n",
        "        mod.load_weights(f'{data_dir}model_{model_name}_{str(model_number+j).zfill(3)}.h5')\n",
        "\n",
        "    # make predictions\n",
        "    if model_type == 'de':\n",
        "        y_pred = []\n",
        "        for model in models:\n",
        "            y_pred.append(model(feat_test.astype('float32')))\n",
        "    elif model_type == 'cd':\n",
        "        y_pred = np.array([models[0].predict(feat_test) for _ in range(n_models)])\n",
        "    elif model_type == 'bnn':\n",
        "        y_pred = [models[0](feat_test.astype('float32')) for _ in range(n_models)]\n",
        "\n",
        "    if model_type == 'de' or model_type == 'bnn':\n",
        "        y_pred_val = [pred.loc.numpy() for pred in y_pred]\n",
        "        y_pred_unc = [pred.scale.numpy() for pred in y_pred]\n",
        "    elif model_type == 'cd':\n",
        "        y_pred_val = y_pred[:, :, :1]\n",
        "        y_pred_unc = np.sqrt(np.exp(y_pred[:, :, 1:]))\n",
        "\n",
        "    y_pred_val_resc = [y_scaler.inverse_transform(y) for y in y_pred_val]\n",
        "    y_pred_unc_resc = [y / y_scaler.scale_[0] for y in y_pred_unc]\n",
        "\n",
        "    y_pred_val_resc = np.array(y_pred_val_resc).reshape((n_models, n_test))\n",
        "    y_pred_unc_resc = np.array(y_pred_unc_resc).reshape((n_models, n_test))\n",
        "\n",
        "    y_pred_mean = np.mean(y_pred_val_resc, axis=0)\n",
        "    y_pred_ep_unc = np.std(y_pred_val_resc, axis=0)\n",
        "    y_pred_al_unc = np.sqrt(np.mean(y_pred_unc_resc * y_pred_unc_resc, axis=0))\n",
        "    y_pred_unc = np.sqrt(y_pred_al_unc ** 2 + y_pred_ep_unc ** 2)\n",
        "\n",
        "    np.save(f'{data_dir}y_pred_test_{model_name}_{str(model_number).zfill(3)}{test_name}.npy', y_pred_mean)\n",
        "    np.save(f'{data_dir}y_pred_test_alunc_{model_name}_{str(model_number).zfill(3)}{test_name}.npy', y_pred_al_unc)\n",
        "    np.save(f'{data_dir}y_pred_test_epunc_{model_name}_{str(model_number).zfill(3)}{test_name}.npy', y_pred_ep_unc)\n",
        "    np.save(f'{data_dir}y_pred_test_prunc_{model_name}_{str(model_number).zfill(3)}{test_name}.npy', y_pred_unc)\n",
        "\n",
        "model_type = ('cd', 'bnn', 'de')\n",
        "model_number = (1, 1, 1)\n",
        "t_spread_min = (0.01, 0.01, 0.01)\n",
        "t_spread_max = (0.1, 0.1, 0.1)\n",
        "ell_spread_min = (0.02, 0.02, 0.02)\n",
        "ell_spread_max = (0.02, 0.02, 0.02)\n",
        "ell_range_min = (2, 2, 2)\n",
        "ell_range_max = (8, 8, 8)\n",
        "g_range_min = (5, 5, 5)\n",
        "g_range_max = (15, 15, 15)\n",
        "n_test = (10000, 10000, 10000)\n",
        "n_epochs = (100, 100, 20)\n",
        "data_dir = ('data/', 'data/', 'data/')\n",
        "\n",
        "for i in range(3):\n",
        "  args = {'model_type': model_type[i],\n",
        "          'model_number': model_number[i],\n",
        "          't_spread_min': t_spread_min[i],\n",
        "          't_spread_max': t_spread_max[i],\n",
        "          'ell_spread_min': ell_spread_min[i],\n",
        "          'ell_spread_max': ell_spread_max[i],\n",
        "          'ell_range_min' : ell_range_min[i],\n",
        "          'ell_range_max' : ell_range_max[i],\n",
        "          'g_range_min' : g_range_min[i],\n",
        "          'g_range_max' : g_range_max[i],\n",
        "          'n_test': n_test[i],\n",
        "          'n_epochs': n_epochs[i],\n",
        "          'data_dir': data_dir[i]}\n",
        "  main(**args)"
      ],
      "metadata": {
        "id": "aKzbjj0WXJtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDxDahbJl93B"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "val_proportion = 0.1\n",
        "n_models = 10\n",
        "n_neurons = 100\n",
        "\n",
        "\n",
        "def main(model_type, t_spread_min, t_spread_max, ell_spread_min, ell_spread_max, n, n_test, n_epochs, data_dir):\n",
        "    # Generate data\n",
        "    feat, y, _, _ = pendulum(n=n, t_spread=[t_spread_min, t_spread_max], ell_spread=[ell_spread_min, ell_spread_max])\n",
        "\n",
        "    # Set up data\n",
        "    x_train, x_val, y_train, y_val = train_test_split(feat, y, test_size=val_proportion, random_state=42)\n",
        "    x_scaler, x_train, x_val = scale(x_train, x_val)\n",
        "    y_scaler, y_train, y_val = scale(y_train, y_val)\n",
        "\n",
        "    t_range_str = f'trange{int(100*t_spread_min)}to{int(100*t_spread_max)}'\n",
        "    model_name = f'{model_type}_{t_range_str}_{n_epochs}ep'\n",
        "\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "    if not os.path.isfile(f'{data_dir}x_scaler_{t_range_str}.pkl'):\n",
        "        with open(f'{data_dir}x_scaler_{t_range_str}.pkl', 'wb') as file_pi:\n",
        "            pickle.dump(x_scaler, file_pi)\n",
        "        with open(f'{data_dir}y_scaler_{t_range_str}.pkl', 'wb') as file_pi:\n",
        "            pickle.dump(y_scaler, file_pi)\n",
        "\n",
        "    # train and save models\n",
        "    model_number = 1\n",
        "    while os.path.isfile(f'{data_dir}model_{model_name}_{str(model_number).zfill(3)}.h5'):\n",
        "        model_number += 1\n",
        "\n",
        "    if model_type == 'de':\n",
        "        models = [mlp(loss='nll') for _ in range(n_models)]\n",
        "    elif model_type == 'cd':\n",
        "        n_features = x_train.shape[1]\n",
        "        n_outputs = y_train.shape[1]\n",
        "        dropout_reg = 2. / n\n",
        "        models = [make_model(n_features, n_outputs, n_neurons, dropout_reg)]\n",
        "    elif model_type == 'bnn':\n",
        "        models = [mlp_flipout()]\n",
        "    else:\n",
        "        raise ValueError(f'Model type {model_type} not recognized!')\n",
        "\n",
        "    for j, mod in enumerate(models):\n",
        "        print(f'Model {j+1}')\n",
        "        history = mod.fit(x_train, y_train, epochs=n_epochs, validation_data=(x_val, y_val))\n",
        "        mod.save_weights(f'{data_dir}model_{model_name}_{str(model_number+j).zfill(3)}.h5')\n",
        "        with open(f'{data_dir}history_{model_name}_{str(model_number+j).zfill(3)}.pkl', 'wb') as file_pi:\n",
        "            pickle.dump(history.history, file_pi)\n",
        "\n",
        "    # Generate test set\n",
        "    feat_test, _, _, _ = pendulum(n=n_test, t_spread=[t_spread_min, t_spread_max],\n",
        "                                  ell_spread=[ell_spread_min, ell_spread_max], seed=666)\n",
        "    feat_test = x_scaler.transform(feat_test)\n",
        "\n",
        "    # make predictions\n",
        "    if model_type == 'de':\n",
        "        y_pred = []\n",
        "        for model in models:\n",
        "            y_pred.append(model(feat_test.astype('float32')))\n",
        "    elif model_type == 'cd':\n",
        "        y_pred = np.array([models[0].predict(feat_test) for _ in range(n_models)])\n",
        "    elif model_type == 'bnn':\n",
        "        y_pred = [models[0](feat_test.astype('float32')) for _ in range(n_models)]\n",
        "\n",
        "    if model_type == 'de' or model_type == 'bnn':\n",
        "        y_pred_val = [pred.loc.numpy() for pred in y_pred]\n",
        "        y_pred_unc = [pred.scale.numpy() for pred in y_pred]\n",
        "    elif model_type == 'cd':\n",
        "        y_pred_val = y_pred[:, :, :1]\n",
        "        y_pred_unc = np.sqrt(np.exp(y_pred[:, :, 1:]))\n",
        "\n",
        "    y_pred_val_resc = [y_scaler.inverse_transform(y) for y in y_pred_val]\n",
        "    y_pred_unc_resc = [y / y_scaler.scale_[0] for y in y_pred_unc]\n",
        "\n",
        "    y_pred_val_resc = np.array(y_pred_val_resc).reshape((n_models, n_test))\n",
        "    y_pred_unc_resc = np.array(y_pred_unc_resc).reshape((n_models, n_test))\n",
        "\n",
        "    y_pred_mean = np.mean(y_pred_val_resc, axis=0)\n",
        "    y_pred_ep_unc = np.std(y_pred_val_resc, axis=0)\n",
        "    y_pred_al_unc = np.sqrt(np.mean(y_pred_unc_resc * y_pred_unc_resc, axis=0))\n",
        "    y_pred_unc = np.sqrt(y_pred_al_unc ** 2 + y_pred_ep_unc ** 2)\n",
        "\n",
        "    np.save(f'{data_dir}y_pred_test_{model_name}_{str(model_number).zfill(3)}.npy', y_pred_mean)\n",
        "    np.save(f'{data_dir}y_pred_test_alunc_{model_name}_{str(model_number).zfill(3)}.npy', y_pred_al_unc)\n",
        "    np.save(f'{data_dir}y_pred_test_epunc_{model_name}_{str(model_number).zfill(3)}.npy', y_pred_ep_unc)\n",
        "    np.save(f'{data_dir}y_pred_test_prunc_{model_name}_{str(model_number).zfill(3)}.npy', y_pred_unc)\n",
        "\n",
        "\n",
        "t_max = (0.05, 0.1, 0.2) # independent of model\n",
        "t_spread_max = (0.1, 0.1, 0.1)\n",
        "\n",
        "model_type = ('cd', 'bnn', 'de')\n",
        "t_spread_min = (0.01, 0.01, 0.01)\n",
        "t_spread_max = (0.2, 0.2, 0.2)\n",
        "ell_spread_min = (0.02, 0.02, 0.02)\n",
        "ell_spread_max = (0.02, 0.02, 0.02)\n",
        "n = (90000, 90000, 90000)\n",
        "n_test = (10000, 10000, 10000)\n",
        "n_epochs = (100, 100, 20)\n",
        "data_dir = ('data/', 'data/', 'data/')\n",
        "for i in range(3):\n",
        "  args = {'model_type': model_type[i],\n",
        "          't_spread_min': t_spread_min[i],\n",
        "          't_spread_max': t_spread_max[i],\n",
        "          'ell_spread_min': ell_spread_min[i],\n",
        "          'ell_spread_max': ell_spread_max[i],\n",
        "          'n': n[i],\n",
        "          'n_test': n_test[i],\n",
        "          'n_epochs': n_epochs[i],\n",
        "          'data_dir': data_dir[i]}\n",
        "  main(**args)\n",
        "%tb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rc-IeHzcWkqT"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "val_proportion = 0.1\n",
        "n_models = 10\n",
        "n_neurons = 50\n",
        "\n",
        "\n",
        "def main(model_type, t_spread_min, t_spread_max, ell_spread_min, ell_spread_max, n, n_test, n_epochs, data_dir):\n",
        "    # Generate data\n",
        "    feat, y, _, _ = pendulum(n=n, t_spread=[t_spread_min, t_spread_max], ell_spread=[ell_spread_min, ell_spread_max])\n",
        "\n",
        "    # Set up data\n",
        "    x_train, x_val, y_train, y_val = train_test_split(feat, y, test_size=val_proportion, random_state=42)\n",
        "    x_scaler, x_train, x_val = scale(x_train, x_val)\n",
        "    y_scaler, y_train, y_val = scale(y_train, y_val)\n",
        "\n",
        "    t_range_str = f'trange{int(100*t_spread_min)}to{int(100*t_spread_max)}'\n",
        "    model_name = f'{model_type}_{t_range_str}_{n_epochs}ep'\n",
        "\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "    if not os.path.isfile(f'{data_dir}x_scaler_{t_range_str}.pkl'):\n",
        "        with open(f'{data_dir}x_scaler_{t_range_str}.pkl', 'wb') as file_pi:\n",
        "            pickle.dump(x_scaler, file_pi)\n",
        "        with open(f'{data_dir}y_scaler_{t_range_str}.pkl', 'wb') as file_pi:\n",
        "            pickle.dump(y_scaler, file_pi)\n",
        "\n",
        "    # train and save models\n",
        "    model_number = 1\n",
        "    while os.path.isfile(f'{data_dir}model_{model_name}_{str(model_number).zfill(3)}.h5'):\n",
        "        model_number += 1\n",
        "\n",
        "    if model_type == 'de':\n",
        "        models = [mlp(loss='nll') for _ in range(n_models)]\n",
        "    elif model_type == 'cd':\n",
        "        n_features = x_train.shape[1]\n",
        "        n_outputs = y_train.shape[1]\n",
        "        dropout_reg = 2. / n\n",
        "        models = [make_model(n_features, n_outputs, n_neurons, dropout_reg)]\n",
        "    elif model_type == 'bnn':\n",
        "        models = [mlp_flipout()]\n",
        "    else:\n",
        "        raise ValueError(f'Model type {model_type} not recognized!')\n",
        "\n",
        "    for j, mod in enumerate(models):\n",
        "        print(f'Model {j+1}')\n",
        "        history = mod.fit(x_train, y_train, epochs=n_epochs, validation_data=(x_val, y_val))\n",
        "        mod.save_weights(f'{data_dir}model_{model_name}_{str(model_number+j).zfill(3)}.h5')\n",
        "        with open(f'{data_dir}history_{model_name}_{str(model_number+j).zfill(3)}.pkl', 'wb') as file_pi:\n",
        "            pickle.dump(history.history, file_pi)\n",
        "\n",
        "    # Generate test set\n",
        "    feat_test, _, _, _ = pendulum(n=n_test, t_spread=[t_spread_min, t_spread_max],\n",
        "                                  ell_spread=[ell_spread_min, ell_spread_max], seed=666)\n",
        "    feat_test = x_scaler.transform(feat_test)\n",
        "\n",
        "    # make predictions\n",
        "    if model_type == 'de':\n",
        "        y_pred = []\n",
        "        for model in models:\n",
        "            y_pred.append(model(feat_test.astype('float32')))\n",
        "    elif model_type == 'cd':\n",
        "        y_pred = np.array([models[0].predict(feat_test) for _ in range(n_models)])\n",
        "    elif model_type == 'bnn':\n",
        "        y_pred = [models[0](feat_test.astype('float32')) for _ in range(n_models)]\n",
        "\n",
        "    if model_type == 'de' or model_type == 'bnn':\n",
        "        y_pred_val = [pred.loc.numpy() for pred in y_pred]\n",
        "        y_pred_unc = [pred.scale.numpy() for pred in y_pred]\n",
        "    elif model_type == 'cd':\n",
        "        y_pred_val = y_pred[:, :, :1]\n",
        "        y_pred_unc = np.sqrt(np.exp(y_pred[:, :, 1:]))\n",
        "\n",
        "    y_pred_val_resc = [y_scaler.inverse_transform(y) for y in y_pred_val]\n",
        "    y_pred_unc_resc = [y / y_scaler.scale_[0] for y in y_pred_unc]\n",
        "\n",
        "    y_pred_val_resc = np.array(y_pred_val_resc).reshape((n_models, n_test))\n",
        "    y_pred_unc_resc = np.array(y_pred_unc_resc).reshape((n_models, n_test))\n",
        "\n",
        "    y_pred_mean = np.mean(y_pred_val_resc, axis=0)\n",
        "    y_pred_ep_unc = np.std(y_pred_val_resc, axis=0)\n",
        "    y_pred_al_unc = np.sqrt(np.mean(y_pred_unc_resc * y_pred_unc_resc, axis=0))\n",
        "    y_pred_unc = np.sqrt(y_pred_al_unc ** 2 + y_pred_ep_unc ** 2)\n",
        "\n",
        "    np.save(f'{data_dir}y_pred_test_{model_name}_{str(model_number).zfill(3)}.npy', y_pred_mean)\n",
        "    np.save(f'{data_dir}y_pred_test_alunc_{model_name}_{str(model_number).zfill(3)}.npy', y_pred_al_unc)\n",
        "    np.save(f'{data_dir}y_pred_test_epunc_{model_name}_{str(model_number).zfill(3)}.npy', y_pred_ep_unc)\n",
        "    np.save(f'{data_dir}y_pred_test_prunc_{model_name}_{str(model_number).zfill(3)}.npy', y_pred_unc)\n",
        "\n",
        "\n",
        "t_max = (0.05, 0.1, 0.2) # independent of model\n",
        "t_spread_max = (0.1, 0.1, 0.1)\n",
        "\n",
        "model_type = ('cd', 'bnn', 'de')\n",
        "t_spread_min = (0.01, 0.01, 0.01)\n",
        "t_spread_max = (0.1, 0.1, 0.1)\n",
        "ell_spread_min = (0.02, 0.02, 0.02)\n",
        "ell_spread_max = (0.02, 0.02, 0.02)\n",
        "n = (90000, 90000, 90000)\n",
        "n_test = (10000, 10000, 10000)\n",
        "n_epochs = (100, 100, 20)\n",
        "data_dir = ('data/', 'data/', 'data/')\n",
        "for i in range(3):\n",
        "  args = {'model_type': model_type[i],\n",
        "          't_spread_min': t_spread_min[i],\n",
        "          't_spread_max': t_spread_max[i],\n",
        "          'ell_spread_min': ell_spread_min[i],\n",
        "          'ell_spread_max': ell_spread_max[i],\n",
        "          'n': n[i],\n",
        "          'n_test': n_test[i],\n",
        "          'n_epochs': n_epochs[i],\n",
        "          'data_dir': data_dir[i]}\n",
        "  main(**args)\n",
        "%tb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "from sklearn.metrics import mean_squared_error\n"
      ],
      "metadata": {
        "id": "ybS6BCEmeLGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import rcParams,rc\n",
        "rc('text', usetex=True)\n",
        "rcParams['font.family'] = 'serif'"
      ],
      "metadata": {
        "id": "F6xyhH_TeSG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def medians_from_scatter(x, y, n_bins=30, skip_end=5):\n",
        "    left_lim = np.amin(x)\n",
        "    right_lim = np.amax(x)\n",
        "    right_lim += (right_lim - left_lim)/100000.\n",
        "    bin_edges = np.linspace(left_lim, right_lim, n_bins+1)\n",
        "    bin_centers = (bin_edges[1:] + bin_edges[:-1])/2.\n",
        "    idxs = [np.logical_and(x >= bin_edges[i], x < bin_edges[i+1]) for i in np.arange(n_bins)]\n",
        "\n",
        "    median = np.array([np.percentile(y[idx], 50) for idx in idxs[:-skip_end]])\n",
        "    lower = np.array([np.percentile(y[idx], 16) for idx in idxs[:-skip_end]])\n",
        "    upper = np.array([np.percentile(y[idx], 84) for idx in idxs[:-skip_end]])\n",
        "\n",
        "    return bin_centers[:-skip_end], median, lower, upper"
      ],
      "metadata": {
        "id": "jtLb8-6ueSyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_coverage(res, unc):\n",
        "    sigma = np.abs(res/unc)\n",
        "    cdf_sigma = 2*norm.cdf(sigma)-1\n",
        "    x = np.linspace(0.01, 1, 100)\n",
        "    coverage = [sum(cdf_sigma <= k)/nobject for k in x]\n",
        "    return coverage"
      ],
      "metadata": {
        "id": "79yDXjJleUxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make a test set\n",
        "nobject=10000\n",
        "\n",
        "feat_test_t1to10, y_test_t1to10, calc_y_test_t1to10, delta_y_test_t1to10 = pendulum(n=nobject, t_spread=[0.01, 0.1],\n",
        "                                                  ell_spread=[0.02, 0.02], seed=666)\n",
        "feat_test_t1to20, y_test_t1to20, calc_y_test_t1to20, delta_y_test_t1to20 = pendulum(n=nobject, t_spread=[0.01, 0.2],\n",
        "                                                  ell_spread=[0.02, 0.02], seed=666)"
      ],
      "metadata": {
        "id": "_wfBaQl7eWgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_de_t1to10 = np.load('data/y_pred_test_de_trange1to10_20ep_001.npy')\n",
        "y_pred_alunc_de_t1to10 = np.load('data/y_pred_test_alunc_de_trange1to10_20ep_001.npy')\n",
        "y_pred_epunc_de_t1to10 = np.load('data/y_pred_test_epunc_de_trange1to10_20ep_001.npy')\n",
        "y_pred_prunc_de_t1to10 = np.load('data/y_pred_test_prunc_de_trange1to10_20ep_001.npy')"
      ],
      "metadata": {
        "id": "sGo7xMuVeYi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_de_t1to20 = np.load('data/y_pred_test_de_trange1to20_20ep_001.npy')\n",
        "y_pred_alunc_de_t1to20 = np.load('data/y_pred_test_alunc_de_trange1to20_20ep_001.npy')\n",
        "y_pred_epunc_de_t1to20 = np.load('data/y_pred_test_epunc_de_trange1to20_20ep_001.npy')\n",
        "y_pred_prunc_de_t1to20 = np.load('data/y_pred_test_prunc_de_trange1to20_20ep_001.npy')"
      ],
      "metadata": {
        "id": "Tef7wdfneZpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_bnn_t1to10 = np.load('data/y_pred_test_bnn_trange1to10_100ep_001.npy')\n",
        "y_pred_alunc_bnn_t1to10 = np.load('data/y_pred_test_alunc_bnn_trange1to10_100ep_001.npy')\n",
        "y_pred_epunc_bnn_t1to10 = np.load('data/y_pred_test_epunc_bnn_trange1to10_100ep_001.npy')\n",
        "y_pred_prunc_bnn_t1to10 = np.load('data/y_pred_test_prunc_bnn_trange1to10_100ep_001.npy')"
      ],
      "metadata": {
        "id": "V4KRM7vMecBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_bnn_t1to20 = np.load('data/y_pred_test_bnn_trange1to20_100ep_001.npy')\n",
        "y_pred_alunc_bnn_t1to20 = np.load('data/y_pred_test_alunc_bnn_trange1to20_100ep_001.npy')\n",
        "y_pred_epunc_bnn_t1to20 = np.load('data/y_pred_test_epunc_bnn_trange1to20_100ep_001.npy')\n",
        "y_pred_prunc_bnn_t1to20 = np.load('data/y_pred_test_prunc_bnn_trange1to20_100ep_001.npy')"
      ],
      "metadata": {
        "id": "ZkLbi3r8edYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_cd_t1to10 = np.load('data/y_pred_test_cd_trange1to10_100ep_001.npy')\n",
        "y_pred_alunc_cd_t1to10 = np.load('data/y_pred_test_alunc_cd_trange1to10_100ep_001.npy')\n",
        "y_pred_epunc_cd_t1to10 = np.load('data/y_pred_test_epunc_cd_trange1to10_100ep_001.npy')\n",
        "y_pred_prunc_cd_t1to10 = np.load('data/y_pred_test_prunc_cd_trange1to10_100ep_001.npy')"
      ],
      "metadata": {
        "id": "h2b8AOj7efvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_cd_t1to20 = np.load('data/y_pred_test_cd_trange1to20_100ep_001.npy')\n",
        "y_pred_alunc_cd_t1to20 = np.load('data/y_pred_test_alunc_cd_trange1to20_100ep_001.npy')\n",
        "y_pred_epunc_cd_t1to20 = np.load('data/y_pred_test_epunc_cd_trange1to20_100ep_001.npy')\n",
        "y_pred_prunc_cd_t1to20 = np.load('data/y_pred_test_prunc_cd_trange1to20_100ep_001.npy')"
      ],
      "metadata": {
        "id": "A7_i-aaPeqIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analytic_t1to10, median_de_t1to10, low_de_t1to10, upp_de_t1to10 = medians_from_scatter(delta_y_test_t1to10/calc_y_test_t1to10,\n",
        "                                                                                       y_pred_alunc_de_t1to10/y_pred_de_t1to10)\n",
        "analytic_t1to20, median_de_t1to20, low_de_t1to20, upp_de_t1to20 = medians_from_scatter(delta_y_test_t1to20/calc_y_test_t1to20,\n",
        "                                                                                       y_pred_alunc_de_t1to20/y_pred_de_t1to20)\n",
        "analytic_t1to5, median_de_t1to5, low_de_t1to5, upp_de_t1to5 = medians_from_scatter(delta_y_test_t1to5/calc_y_test_t1to5,\n",
        "                                                                                   y_pred_alunc_de_t1to5/y_pred_de_t1to5)"
      ],
      "metadata": {
        "id": "UmLSemCaimJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, median_bnn_t1to10, low_bnn_t1to10, upp_bnn_t1to10 = medians_from_scatter(delta_y_test_t1to10/calc_y_test_t1to10,\n",
        "                                                                            y_pred_alunc_bnn_t1to10/y_pred_bnn_t1to10)\n",
        "_, median_bnn_t1to20, low_bnn_t1to20, upp_bnn_t1to20 = medians_from_scatter(delta_y_test_t1to20/calc_y_test_t1to20,\n",
        "                                                                            y_pred_alunc_bnn_t1to20/y_pred_bnn_t1to20)\n",
        "_, median_bnn_t1to5, low_bnn_t1to5, upp_bnn_t1to5 = medians_from_scatter(delta_y_test_t1to5/calc_y_test_t1to5,\n",
        "                                                                         y_pred_alunc_bnn_t1to5/y_pred_bnn_t1to5)"
      ],
      "metadata": {
        "id": "K0YR3lDPimu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, median_cd_t1to10, low_cd_t1to10, upp_cd_t1to10 = medians_from_scatter(delta_y_test_t1to10/calc_y_test_t1to10,\n",
        "                                                                         y_pred_alunc_cd_t1to10/y_pred_cd_t1to10)\n",
        "_, median_cd_t1to20, low_cd_t1to20, upp_cd_t1to20 = medians_from_scatter(delta_y_test_t1to20/calc_y_test_t1to20,\n",
        "                                                                         y_pred_alunc_cd_t1to20/y_pred_cd_t1to20)\n",
        "_, median_cd_t1to5, low_cd_t1to5, upp_cd_t1to5 = medians_from_scatter(delta_y_test_t1to5/calc_y_test_t1to5,\n",
        "                                                                      y_pred_alunc_cd_t1to5/y_pred_cd_t1to5)"
      ],
      "metadata": {
        "id": "25C_F4b1inyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fontsize=14\n",
        "fontsize_title=16\n",
        "\n",
        "plt.figure(figsize=(14,4.5))\n",
        "\n",
        "ax0 = plt.subplot(132)\n",
        "plt.plot(analytic_t1to10, median_de_t1to10, alpha=0.5, c='#000080', label='DE')\n",
        "plt.fill_between(analytic_t1to10, low_de_t1to10, upp_de_t1to10,\n",
        "                facecolor='#000080', alpha=0.2)\n",
        "plt.plot(analytic_t1to10, median_cd_t1to10, alpha=0.5, c='#800080', label='CD')\n",
        "plt.fill_between(analytic_t1to10, low_cd_t1to10, upp_cd_t1to10,\n",
        "                facecolor='#800080', alpha=0.2)\n",
        "plt.plot(analytic_t1to10, median_bnn_t1to10, alpha=0.5, c='#FF0000', label='BNN')\n",
        "plt.fill_between(analytic_t1to10, low_bnn_t1to10, upp_bnn_t1to10,\n",
        "                facecolor='#FF0000', alpha=0.2)\n",
        "start_pt = 0.025\n",
        "end_pt = 0.07\n",
        "plt.plot([start_pt, end_pt], [start_pt, end_pt], '--', color=(0,0,0))\n",
        "plt.xlim([start_pt, end_pt])\n",
        "plt.ylim([start_pt, end_pt])\n",
        "plt.legend(fontsize=fontsize)\n",
        "plt.xticks(fontsize=fontsize)\n",
        "plt.yticks(fontsize=fontsize)\n",
        "plt.title('Trained with T noise of 1-10\\%', fontsize=fontsize_title)\n",
        "plt.xlabel('Relative analytic uncertainty estimates', fontsize=fontsize)\n",
        "\n",
        "ax0 = plt.subplot(133)\n",
        "plt.plot(analytic_t1to20, median_de_t1to20, alpha=0.5, c='#000080', label='DE')\n",
        "plt.fill_between(analytic_t1to20, low_de_t1to20, upp_de_t1to20,\n",
        "                facecolor='#000080', alpha=0.2)\n",
        "plt.plot(analytic_t1to20, median_cd_t1to20, alpha=0.5, c='#800080', label='CD')\n",
        "plt.fill_between(analytic_t1to20, low_cd_t1to20, upp_cd_t1to20,\n",
        "                facecolor='#800080', alpha=0.2)\n",
        "plt.plot(analytic_t1to20, median_bnn_t1to20, alpha=0.5, c='#FF0000', label='BNN')\n",
        "plt.fill_between(analytic_t1to20, low_bnn_t1to20, upp_bnn_t1to20,\n",
        "                facecolor='#FF0000', alpha=0.2)\n",
        "start_pt = 0.025\n",
        "end_pt = 0.14\n",
        "plt.plot([start_pt, end_pt], [start_pt, end_pt], '--', color=(0,0,0))\n",
        "plt.xlim([start_pt, end_pt])\n",
        "plt.ylim([start_pt, end_pt])\n",
        "plt.legend(fontsize=fontsize)\n",
        "plt.xticks(fontsize=fontsize)\n",
        "plt.yticks(fontsize=fontsize)\n",
        "plt.title('Trained with T noise of 1-20\\%', fontsize=fontsize_title)\n",
        "plt.xlabel('Relative analytic uncertainty estimates', fontsize=fontsize)\n",
        "plt.tight_layout()\n",
        "plt.savefig('comparisons_diffnoise_perc.pdf', fmt='pdf', dpi=500)\n",
        "plt.savefig('comparisons_diffnoise_perc.png', fmt='png', dpi=500)"
      ],
      "metadata": {
        "id": "D5LShpVUjW08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_bnn_oodl = []\n",
        "y_pred_alunc_bnn_oodl = []\n",
        "y_pred_epunc_bnn_oodl = []\n",
        "y_pred_prunc_bnn_oodl = []\n",
        "for i in range(2, 16):\n",
        "    y_pred_bnn_oodl.append(np.load(f'data/y_pred_test_bnn_trange1to20_200ep_001_ell{i}to{i+1}.npy'))\n",
        "    y_pred_alunc_bnn_oodl.append(np.load(f'data/y_pred_test_alunc_bnn_trange1to20_200ep_001_ell{i}to{i+1}.npy'))\n",
        "    y_pred_epunc_bnn_oodl.append(np.load(f'data/y_pred_test_epunc_bnn_trange1to20_200ep_001_ell{i}to{i+1}.npy'))\n",
        "    y_pred_prunc_bnn_oodl.append(np.load(f'data/y_pred_test_prunc_bnn_trange1to20_200ep_001_ell{i}to{i+1}.npy'))\n",
        "\n",
        "y_pred_de_oodl = []\n",
        "y_pred_alunc_de_oodl = []\n",
        "y_pred_epunc_de_oodl = []\n",
        "y_pred_prunc_de_oodl = []\n",
        "for i in range(2, 16):\n",
        "    y_pred_de_oodl.append(np.load(f'data/y_pred_test_de_trange1to20_40ep_001_ell{i}to{i+1}.npy'))\n",
        "    y_pred_alunc_de_oodl.append(np.load(f'data/y_pred_test_alunc_de_trange1to20_40ep_001_ell{i}to{i+1}.npy'))\n",
        "    y_pred_epunc_de_oodl.append(np.load(f'data/y_pred_test_epunc_de_trange1to20_40ep_001_ell{i}to{i+1}.npy'))\n",
        "    y_pred_prunc_de_oodl.append(np.load(f'data/y_pred_test_prunc_de_trange1to20_40ep_001_ell{i}to{i+1}.npy'))\n",
        "\n",
        "y_pred_cd_oodl = []\n",
        "y_pred_alunc_cd_oodl = []\n",
        "y_pred_epunc_cd_oodl = []\n",
        "y_pred_prunc_cd_oodl = []\n",
        "for i in range(2, 16):\n",
        "    y_pred_cd_oodl.append(np.load(f'data/y_pred_test_cd_trange1to20_200ep_001_ell{i}to{i+1}.npy'))\n",
        "    y_pred_alunc_cd_oodl.append(np.load(f'data/y_pred_test_alunc_cd_trange1to20_200ep_001_ell{i}to{i+1}.npy'))\n",
        "    y_pred_epunc_cd_oodl.append(np.load(f'data/y_pred_test_epunc_cd_trange1to20_200ep_001_ell{i}to{i+1}.npy'))\n",
        "    y_pred_prunc_cd_oodl.append(np.load(f'data/y_pred_test_prunc_cd_trange1to20_200ep_001_ell{i}to{i+1}.npy'))"
      ],
      "metadata": {
        "id": "60bgA_C5jGHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_epunc_bnn_oodl_50 = np.percentile(y_pred_epunc_bnn_oodl, 50, axis=1)\n",
        "y_pred_epunc_cd_oodl_50 = np.percentile(y_pred_epunc_cd_oodl, 50, axis=1)\n",
        "y_pred_epunc_de_oodl_50 = np.percentile(y_pred_epunc_de_oodl, 50, axis=1)\n",
        "y_pred_epunc_bnn_oodl_16 = np.percentile(y_pred_epunc_bnn_oodl, 16, axis=1)\n",
        "y_pred_epunc_cd_oodl_16 = np.percentile(y_pred_epunc_cd_oodl, 16, axis=1)\n",
        "y_pred_epunc_de_oodl_16 = np.percentile(y_pred_epunc_de_oodl, 16, axis=1)\n",
        "y_pred_epunc_bnn_oodl_84 = np.percentile(y_pred_epunc_bnn_oodl, 84, axis=1)\n",
        "y_pred_epunc_cd_oodl_84 = np.percentile(y_pred_epunc_cd_oodl, 84, axis=1)\n",
        "y_pred_epunc_de_oodl_84 = np.percentile(y_pred_epunc_de_oodl, 84, axis=1)"
      ],
      "metadata": {
        "id": "vuMXidNMjG3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fontsize=14\n",
        "fontsize_title=16\n",
        "\n",
        "x = np.arange(0.25, 1.6, 0.1)\n",
        "plt.plot(x, y_pred_epunc_de_oodl_50, alpha=0.5, c='#000080', label='DE')\n",
        "plt.fill_between(x, y_pred_epunc_de_oodl_16, y_pred_epunc_de_oodl_84,\n",
        "                facecolor='#000080', alpha=0.2)\n",
        "plt.plot(x, y_pred_epunc_cd_oodl_50, alpha=0.5, c='#800080', label='CD')\n",
        "plt.fill_between(x, y_pred_epunc_cd_oodl_16, y_pred_epunc_cd_oodl_84,\n",
        "                facecolor='#800080', alpha=0.2)\n",
        "plt.plot(x, y_pred_epunc_bnn_oodl_50, alpha=0.5, c='#FF0000', label='BNN')\n",
        "plt.fill_between(x, y_pred_epunc_bnn_oodl_16, y_pred_epunc_bnn_oodl_84,\n",
        "                facecolor='#FF0000', alpha=0.2)\n",
        "plt.axvline(0.2, ls='--', c='black', linewidth=1)\n",
        "plt.axvline(0.8, ls='--', c='black', linewidth=1, label='Training range')\n",
        "\n",
        "plt.legend(fontsize=fontsize, loc=2)\n",
        "plt.xticks(fontsize=fontsize)\n",
        "plt.yticks(fontsize=fontsize)\n",
        "plt.xlabel('Input $L$ (m)', fontsize=fontsize)\n",
        "plt.ylabel('Epistemic uncertainty (m/s$^2$)', fontsize=fontsize)\n",
        "plt.tight_layout()\n",
        "plt.savefig('oodl.pdf', fmt='pdf', dpi=500)\n",
        "plt.savefig('oodl.png', fmt='png', dpi=500)"
      ],
      "metadata": {
        "id": "H62LqPzxjKDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fontsize=14\n",
        "fontsize_title=16\n",
        "\n",
        "plt.figure(figsize=(14,4.5))\n",
        "\n",
        "ax0 = plt.subplot(132)\n",
        "plt.plot(analytic_t1to10, median_de_t1to10, alpha=0.5, c='#000080', label='DE')\n",
        "plt.fill_between(analytic_t1to10, low_de_t1to10, upp_de_t1to10,\n",
        "                facecolor='#000080', alpha=0.2)\n",
        "plt.plot(analytic_t1to10, median_cd_t1to10, alpha=0.5, c='#800080', label='CD')\n",
        "plt.fill_between(analytic_t1to10, low_cd_t1to10, upp_cd_t1to10,\n",
        "                facecolor='#800080', alpha=0.2)\n",
        "plt.plot(analytic_t1to10, median_bnn_t1to10, alpha=0.5, c='#FF0000', label='BNN')\n",
        "plt.fill_between(analytic_t1to10, low_bnn_t1to10, upp_bnn_t1to10,\n",
        "                facecolor='#FF0000', alpha=0.2)\n",
        "start_pt = 0.025\n",
        "end_pt = 0.07\n",
        "plt.plot([start_pt, end_pt], [start_pt, end_pt], '--', color=(0,0,0))\n",
        "plt.xlim([start_pt, end_pt])\n",
        "plt.ylim([start_pt, end_pt])\n",
        "plt.legend(fontsize=fontsize)\n",
        "plt.xticks(fontsize=fontsize)\n",
        "plt.yticks(fontsize=fontsize)\n",
        "plt.title('Trained with T noise of 1-10\\%', fontsize=fontsize_title)\n",
        "plt.xlabel('Relative analytic uncertainty estimates', fontsize=fontsize)\n",
        "\n",
        "ax0 = plt.subplot(133)\n",
        "plt.plot(analytic_t1to20, median_de_t1to20, alpha=0.5, c='#000080', label='DE')\n",
        "plt.fill_between(analytic_t1to20, low_de_t1to20, upp_de_t1to20,\n",
        "                facecolor='#000080', alpha=0.2)\n",
        "plt.plot(analytic_t1to20, median_cd_t1to20, alpha=0.5, c='#800080', label='CD')\n",
        "plt.fill_between(analytic_t1to20, low_cd_t1to20, upp_cd_t1to20,\n",
        "                facecolor='#800080', alpha=0.2)\n",
        "plt.plot(analytic_t1to20, median_bnn_t1to20, alpha=0.5, c='#FF0000', label='BNN')\n",
        "plt.fill_between(analytic_t1to20, low_bnn_t1to20, upp_bnn_t1to20,\n",
        "                facecolor='#FF0000', alpha=0.2)\n",
        "start_pt = 0.025\n",
        "end_pt = 0.14\n",
        "plt.plot([start_pt, end_pt], [start_pt, end_pt], '--', color=(0,0,0))\n",
        "plt.xlim([start_pt, end_pt])\n",
        "plt.ylim([start_pt, end_pt])\n",
        "plt.legend(fontsize=fontsize)\n",
        "plt.xticks(fontsize=fontsize)\n",
        "plt.yticks(fontsize=fontsize)\n",
        "plt.title('Trained with T noise of 1-20\\%', fontsize=fontsize_title)\n",
        "plt.xlabel('Relative analytic uncertainty estimates', fontsize=fontsize)\n",
        "plt.tight_layout()\n",
        "plt.savefig('comparisons_diffnoise_perc.pdf', fmt='pdf', dpi=500)\n",
        "plt.savefig('comparisons_diffnoise_perc.png', fmt='png', dpi=500)"
      ],
      "metadata": {
        "id": "JUYBmLU3i4eL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "residuals_de = y_pred_de_t1to20 - y_test_t1to20\n",
        "residuals_cd = y_pred_cd_t1to20 - y_test_t1to20\n",
        "residuals_bnn = y_pred_bnn_t1to20 - y_test_t1to20"
      ],
      "metadata": {
        "id": "VlV5Oi2ijgr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coverage_de = find_coverage(residuals_de, y_pred_prunc_de_t1to20)\n",
        "coverage_cd = find_coverage(residuals_cd, y_pred_prunc_cd_t1to20)\n",
        "coverage_bnn = find_coverage(residuals_bnn, y_pred_prunc_bnn_t1to20)"
      ],
      "metadata": {
        "id": "dJEdjT5EjrZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_de_t1to20_8to12 = np.load('data/y_pred_test_de_trange1to20_20ep_001_ell8to12.npy')\n",
        "y_pred_prunc_de_t1to20_8to12 = np.load('data/y_pred_test_prunc_de_trange1to20_20ep_001_ell8to12.npy')\n",
        "\n",
        "y_pred_cd_t1to20_8to12 = np.load('data/y_pred_test_cd_trange1to20_100ep_001_ell8to12.npy')\n",
        "y_pred_prunc_cd_t1to20_8to12 = np.load('data/y_pred_test_prunc_cd_trange1to20_100ep_001_ell8to12.npy')\n",
        "\n",
        "y_pred_bnn_t1to20_8to12 = np.load('data/y_pred_test_bnn_trange1to20_100ep_001_ell8to12.npy')\n",
        "y_pred_prunc_bnn_t1to20_8to12 = np.load('data//y_pred_test_prunc_bnn_trange1to20_100ep_001_ell8to12.npy')"
      ],
      "metadata": {
        "id": "KYMqqQmijsaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_de_t1to20_12to16 = np.load('data/y_pred_test_de_trange1to20_20ep_001_ell12to16.npy')\n",
        "y_pred_prunc_de_t1to20_12to16 = np.load('data/y_pred_test_prunc_de_trange1to20_20ep_001_ell12to16.npy')\n",
        "\n",
        "y_pred_cd_t1to20_12to16 = np.load('data/y_pred_test_cd_trange1to20_100ep_001_ell12to16.npy')\n",
        "y_pred_prunc_cd_t1to20_12to16 = np.load('data/y_pred_test_prunc_cd_trange1to20_100ep_001_ell12to16.npy')\n",
        "\n",
        "y_pred_bnn_t1to20_12to16 = np.load('data/y_pred_test_bnn_trange1to20_100ep_001_ell12to16.npy')\n",
        "y_pred_prunc_bnn_t1to20_12to16 = np.load('data//y_pred_test_prunc_bnn_trange1to20_100ep_001_ell12to16.npy')"
      ],
      "metadata": {
        "id": "qkTKc4ZDjxVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feat_test_t1to20_8to12, y_test_t1to20_8to12, calc_y_test_t1to20_8to12, delta_y_test_t1to20_8to12 = pendulum(n=nobject, t_spread=[0.01, 0.2],\n",
        "                                                  ell_spread=[0.02, 0.02], seed=666, ell_range=[0.8, 1.2])"
      ],
      "metadata": {
        "id": "hVisCls1j2iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feat_test_t1to20_12to16, y_test_t1to20_12to16, calc_y_test_t1to20_12to16, delta_y_test_t1to20_12to16 = pendulum(n=nobject, t_spread=[0.01, 0.2],\n",
        "                                                  ell_spread=[0.02, 0.02], seed=666, ell_range=[1.2, 1.6])"
      ],
      "metadata": {
        "id": "wk9jYXxmj5Np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "residuals_de_8to12 = y_pred_de_t1to20_8to12 - y_test_t1to20_8to12\n",
        "residuals_cd_8to12 = y_pred_cd_t1to20_8to12 - y_test_t1to20_8to12\n",
        "residuals_bnn_8to12 = y_pred_bnn_t1to20_8to12 - y_test_t1to20_8to12\n",
        "\n",
        "residuals_de_12to16 = y_pred_de_t1to20_12to16 - y_test_t1to20_12to16\n",
        "residuals_cd_12to16 = y_pred_cd_t1to20_12to16 - y_test_t1to20_12to16\n",
        "residuals_bnn_12to16 = y_pred_bnn_t1to20_12to16 - y_test_t1to20_12to16"
      ],
      "metadata": {
        "id": "_FmOT4SQj7FZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coverage_de_8to12 = find_coverage(residuals_de_8to12, y_pred_prunc_de_t1to20_8to12)\n",
        "coverage_cd_8to12 = find_coverage(residuals_cd_8to12, y_pred_prunc_cd_t1to20_8to12)\n",
        "coverage_bnn_8to12 = find_coverage(residuals_bnn_8to12, y_pred_prunc_bnn_t1to20_8to12)\n",
        "\n",
        "coverage_de_12to16 = find_coverage(residuals_de_12to16, y_pred_prunc_de_t1to20_12to16)\n",
        "coverage_cd_12to16 = find_coverage(residuals_cd_12to16, y_pred_prunc_cd_t1to20_12to16)\n",
        "coverage_bnn_12to16 = find_coverage(residuals_bnn_12to16, y_pred_prunc_bnn_t1to20_12to16)"
      ],
      "metadata": {
        "id": "gAS7093Cj8hN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.linspace(0.01, 1, 100)\n",
        "\n",
        "fontsize=14\n",
        "fontsize_title=16\n",
        "\n",
        "plt.figure(figsize=(13,4))\n",
        "\n",
        "ax0 = plt.subplot(131)\n",
        "plt.plot(x, x, ls='--', c=(0,0,0))\n",
        "plt.plot(x, coverage_de, label='DE', c='#000080', alpha=0.5)\n",
        "plt.plot(x, coverage_cd, label='CD', c='#800080', alpha=0.5)\n",
        "plt.plot(x, coverage_bnn, label='BNN', c='#FF0000', alpha=0.5)\n",
        "plt.legend(fontsize=fontsize)\n",
        "plt.xlabel('Conf. int. size assuming normal dist.', fontsize=fontsize)\n",
        "plt.ylabel('Proportion of points inside interval', fontsize=fontsize)\n",
        "plt.xticks(fontsize=fontsize)\n",
        "plt.yticks(fontsize=fontsize)\n",
        "plt.title('L in 0.2 to 0.8 m', fontsize=fontsize_title)\n",
        "\n",
        "ax0 = plt.subplot(132)\n",
        "plt.plot(x, x, ls='--', c=(0,0,0))\n",
        "plt.plot(x, coverage_de_8to12, label='DE', c='#000080', alpha=0.5)\n",
        "plt.plot(x, coverage_cd_8to12, label='CD', c='#800080', alpha=0.5)\n",
        "plt.plot(x, coverage_bnn_8to12, label='BNN', c='#FF0000', alpha=0.5)\n",
        "plt.legend(fontsize=fontsize)\n",
        "plt.xlabel('Conf. int. size assuming normal dist.', fontsize=fontsize)\n",
        "plt.setp(ax0.get_yticklabels(), visible=False)\n",
        "plt.xticks(fontsize=fontsize)\n",
        "plt.yticks(fontsize=fontsize)\n",
        "plt.title('L in 0.8 to 1.2 m', fontsize=fontsize_title)\n",
        "plt.subplots_adjust(wspace=0., hspace=0.)\n",
        "\n",
        "ax0 = plt.subplot(133)\n",
        "plt.plot(x, x, ls='--', c=(0,0,0))\n",
        "plt.plot(x, coverage_de_12to16, label='DE', c='#000080', alpha=0.5)\n",
        "plt.plot(x, coverage_cd_12to16, label='CD', c='#800080', alpha=0.5)\n",
        "plt.plot(x, coverage_bnn_12to16, label='BNN', c='#FF0000', alpha=0.5)\n",
        "plt.legend(fontsize=fontsize)\n",
        "plt.xlabel('Conf. int. size assuming normal dist.', fontsize=fontsize)\n",
        "plt.setp(ax0.get_yticklabels(), visible=False)\n",
        "plt.xticks(fontsize=fontsize)\n",
        "plt.yticks(fontsize=fontsize)\n",
        "plt.title('L in 1.2 to 1.6 m', fontsize=fontsize_title)\n",
        "plt.tight_layout()\n",
        "plt.savefig('calibration_changingell.pdf', fmt='pdf', dpi=500)\n",
        "plt.savefig('calibration_changingell.png', fmt='png', dpi=500)"
      ],
      "metadata": {
        "id": "yQgdOU_vj9hU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_bnn_oodg = []\n",
        "y_pred_alunc_bnn_oodg = []\n",
        "y_pred_epunc_bnn_oodg = []\n",
        "y_pred_prunc_bnn_oodg = []\n",
        "for i in range(10, 25):\n",
        "    y_pred_bnn_oodg.append(np.load(f'data/y_pred_test_bnn_trange1to20_100ep_001_g{i}to{i+1}.npy'))\n",
        "    y_pred_alunc_bnn_oodg.append(np.load(f'data/y_pred_test_alunc_bnn_trange1to20_100ep_001_g{i}to{i+1}.npy'))\n",
        "    y_pred_epunc_bnn_oodg.append(np.load(f'data/y_pred_test_epunc_bnn_trange1to20_100ep_001_g{i}to{i+1}.npy'))\n",
        "    y_pred_prunc_bnn_oodg.append(np.load(f'data/y_pred_test_prunc_bnn_trange1to20_100ep_001_g{i}to{i+1}.npy'))\n",
        "\n",
        "y_pred_de_oodg = []\n",
        "y_pred_alunc_de_oodg = []\n",
        "y_pred_epunc_de_oodg = []\n",
        "y_pred_prunc_de_oodg = []\n",
        "for i in range(10, 25):\n",
        "    y_pred_de_oodg.append(np.load(f'data/y_pred_test_de_trange1to20_20ep_001_g{i}to{i+1}.npy'))\n",
        "    y_pred_alunc_de_oodg.append(np.load(f'data/y_pred_test_alunc_de_trange1to20_20ep_001_g{i}to{i+1}.npy'))\n",
        "    y_pred_epunc_de_oodg.append(np.load(f'data/y_pred_test_epunc_de_trange1to20_20ep_001_g{i}to{i+1}.npy'))\n",
        "    y_pred_prunc_de_oodg.append(np.load(f'data/y_pred_test_prunc_de_trange1to20_20ep_001_g{i}to{i+1}.npy'))\n",
        "\n",
        "y_pred_cd_oodg = []\n",
        "y_pred_alunc_cd_oodg = []\n",
        "y_pred_epunc_cd_oodg = []\n",
        "y_pred_prunc_cd_oodg = []\n",
        "for i in range(10, 25):\n",
        "    y_pred_cd_oodg.append(np.load(f'data/y_pred_test_cd_trange1to20_100ep_001_g{i}to{i+1}.npy'))\n",
        "    y_pred_alunc_cd_oodg.append(np.load(f'data/y_pred_test_alunc_cd_trange1to20_100ep_001_g{i}to{i+1}.npy'))\n",
        "    y_pred_epunc_cd_oodg.append(np.load(f'data/y_pred_test_epunc_cd_trange1to20_100ep_001_g{i}to{i+1}.npy'))\n",
        "    y_pred_prunc_cd_oodg.append(np.load(f'data/y_pred_test_prunc_cd_trange1to20_100ep_001_g{i}to{i+1}.npy'))"
      ],
      "metadata": {
        "id": "SatgoIGYkBSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_epunc_bnn_oodg_50 = np.percentile(y_pred_epunc_bnn_oodg, 50, axis=1)\n",
        "y_pred_epunc_cd_oodg_50 = np.percentile(y_pred_epunc_cd_oodg, 50, axis=1)\n",
        "y_pred_epunc_de_oodg_50 = np.percentile(y_pred_epunc_de_oodg, 50, axis=1)\n",
        "y_pred_epunc_bnn_oodg_16 = np.percentile(y_pred_epunc_bnn_oodg, 16, axis=1)\n",
        "y_pred_epunc_cd_oodg_16 = np.percentile(y_pred_epunc_cd_oodg, 16, axis=1)\n",
        "y_pred_epunc_de_oodg_16 = np.percentile(y_pred_epunc_de_oodg, 16, axis=1)\n",
        "y_pred_epunc_bnn_oodg_84 = np.percentile(y_pred_epunc_bnn_oodg, 84, axis=1)\n",
        "y_pred_epunc_cd_oodg_84 = np.percentile(y_pred_epunc_cd_oodg, 84, axis=1)\n",
        "y_pred_epunc_de_oodg_84 = np.percentile(y_pred_epunc_de_oodg, 84, axis=1)"
      ],
      "metadata": {
        "id": "6Ph2d4LIkKW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fontsize=14\n",
        "fontsize_title=16\n",
        "\n",
        "x = np.arange(10.5, 25, 1.)\n",
        "plt.plot(x, y_pred_epunc_de_oodg_50, alpha=0.5, c='#000080', label='DE')\n",
        "plt.fill_between(x, y_pred_epunc_de_oodg_16, y_pred_epunc_de_oodg_84,\n",
        "                facecolor='#000080', alpha=0.2)\n",
        "plt.plot(x, y_pred_epunc_cd_oodg_50, alpha=0.5, c='#800080', label='CD')\n",
        "plt.fill_between(x, y_pred_epunc_cd_oodg_16, y_pred_epunc_cd_oodg_84,\n",
        "                facecolor='#800080', alpha=0.2)\n",
        "plt.plot(x, y_pred_epunc_bnn_oodg_50, alpha=0.5, c='#FF0000', label='BNN')\n",
        "plt.fill_between(x, y_pred_epunc_bnn_oodg_16, y_pred_epunc_bnn_oodg_84,\n",
        "                facecolor='#FF0000', alpha=0.2)\n",
        "plt.axvline(15, ls='--', c='black', linewidth=1, label='Training range')\n",
        "\n",
        "plt.legend(fontsize=fontsize, loc=2)\n",
        "plt.xticks(fontsize=fontsize)\n",
        "plt.yticks(fontsize=fontsize)\n",
        "plt.xlabel('Output $g$ (m/s$^2$)', fontsize=fontsize)\n",
        "plt.ylabel('Epistemic uncertainty (m/s$^2$)', fontsize=fontsize)\n",
        "plt.tight_layout()\n",
        "plt.savefig('oodg.pdf', fmt='pdf', dpi=500)\n",
        "plt.savefig('oodg.png', fmt='png', dpi=500)"
      ],
      "metadata": {
        "id": "bhWGnth3kMlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))"
      ],
      "metadata": {
        "id": "9TWnswU1kO7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = make_model(13, 1, 100)\n",
        "model.load_weights('data/model_cd_trange1to20_200ep_001.h5')"
      ],
      "metadata": {
        "id": "At34L1a_kQJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Dropout probabilities: ', np.array([sigmoid(layer.p_logit.numpy()) for layer in model.layers if hasattr(layer, 'p_logit')]).flatten())"
      ],
      "metadata": {
        "id": "mYzx8-eekUM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr_cd_1to10 = np.corrcoef(delta_y_test_t1to10/calc_y_test_t1to10, y_pred_alunc_cd_t1to10/y_pred_cd_t1to10)[0, 1]\n",
        "corr_cd_1to20 = np.corrcoef(delta_y_test_t1to20/calc_y_test_t1to20, y_pred_alunc_cd_t1to20/y_pred_cd_t1to20)[0, 1]\n",
        "corr_de_1to10 = np.corrcoef(delta_y_test_t1to10/calc_y_test_t1to10, y_pred_alunc_de_t1to10/y_pred_de_t1to10)[0, 1]\n",
        "corr_de_1to20 = np.corrcoef(delta_y_test_t1to20/calc_y_test_t1to20, y_pred_alunc_de_t1to20/y_pred_de_t1to20)[0, 1]\n",
        "corr_bnn_1to10 = np.corrcoef(delta_y_test_t1to10/calc_y_test_t1to10, y_pred_alunc_bnn_t1to10/y_pred_bnn_t1to10)[0, 1]\n",
        "corr_bnn_1to20 = np.corrcoef(delta_y_test_t1to20/calc_y_test_t1to20, y_pred_alunc_bnn_t1to20/y_pred_bnn_t1to20)[0, 1]"
      ],
      "metadata": {
        "id": "OJ6mNpqJkVW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.linspace(0.01, 1, 100)\n",
        "coverage_err_cd_2to8 = np.sum(np.abs([coverage_cd-x]))/100\n",
        "coverage_err_de_2to8 = np.sum(np.abs([coverage_de-x]))/100\n",
        "coverage_err_bnn_2to8 = np.sum(np.abs([coverage_bnn-x]))/100\n",
        "coverage_err_cd_8to12 = np.sum(np.abs([coverage_cd_8to12-x]))/100\n",
        "coverage_err_de_8to12 = np.sum(np.abs([coverage_de_8to12-x]))/100\n",
        "coverage_err_bnn_8to12 = np.sum(np.abs([coverage_bnn_8to12-x]))/100\n",
        "coverage_err_cd_12to16 = np.sum(np.abs([coverage_cd_12to16-x]))/100\n",
        "coverage_err_de_12to16 = np.sum(np.abs([coverage_de_12to16-x]))/100\n",
        "coverage_err_bnn_12to16 = np.sum(np.abs([coverage_bnn_12to16-x]))/100"
      ],
      "metadata": {
        "id": "enlZbzBlkZ0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse_cd = mean_squared_error(y_test_t1to20, y_pred_cd_t1to20)\n",
        "mse_de = mean_squared_error(y_test_t1to20, y_pred_de_t1to20)\n",
        "mse_bnn = mean_squared_error(y_test_t1to20, y_pred_bnn_t1to20)"
      ],
      "metadata": {
        "id": "lBnzJMf7kb9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(' & '.join(['', '1-10\\%', '1-20\\%', '', '(0.2, 0.8) m', '(0.8, 1.2) m', '(1.2, 1.6) m', '\\\\\\\\']))\n",
        "print(' & '.join(['DE', '%.2f' % corr_de_1to10, '%.2f' % corr_de_1to20, '', '%.3f' % coverage_err_de_2to8,\n",
        "                  '%.3f' % coverage_err_de_8to12, '%.3f' % coverage_err_de_12to16, '%.2f \\\\\\\\' % mse_de]))\n",
        "print(' & '.join(['CD', '%.2f' % corr_cd_1to10, '%.2f' % corr_cd_1to20, '', '%.3f' % coverage_err_cd_2to8,\n",
        "                  '%.2f' % coverage_err_cd_8to12, '%.2f' % coverage_err_cd_12to16, '%.2f \\\\\\\\' % mse_cd]))\n",
        "print(' & '.join(['BNN', '%.2f' % corr_bnn_1to10, '%.2f' % corr_bnn_1to20, '', '%.3f' % coverage_err_bnn_2to8,\n",
        "                  '%.3f' % coverage_err_bnn_8to12, '%.3f' % coverage_err_bnn_12to16, '%.2f \\\\\\\\' % mse_bnn]))"
      ],
      "metadata": {
        "id": "xIXkQvPbkdTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "elmawjGxl0G3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}